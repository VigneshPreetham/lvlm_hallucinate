{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d6fd342-63cc-42bd-b14f-398402943b38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T18:01:00.978028Z",
     "iopub.status.busy": "2024-12-07T18:01:00.978028Z",
     "iopub.status.idle": "2024-12-07T18:01:06.714311Z",
     "shell.execute_reply": "2024-12-07T18:01:06.714311Z",
     "shell.execute_reply.started": "2024-12-07T18:01:00.978028Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n",
    "import torch\n",
    "import json\n",
    "from PIL import Image\n",
    "import requests\n",
    "from datasets import load_dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "660d73c3-902d-4278-b655-1507aaf21b74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T18:01:07.468373Z",
     "iopub.status.busy": "2024-12-07T18:01:07.468373Z",
     "iopub.status.idle": "2024-12-07T18:01:07.471322Z",
     "shell.execute_reply": "2024-12-07T18:01:07.471322Z",
     "shell.execute_reply.started": "2024-12-07T18:01:07.468373Z"
    }
   },
   "outputs": [],
   "source": [
    "output_path = os.path.join(os.curdir, \"model_responses.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e40994c-cce3-473d-bb05-8e14372a207e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T18:01:10.044982Z",
     "iopub.status.busy": "2024-12-07T18:01:10.044982Z",
     "iopub.status.idle": "2024-12-07T18:01:10.047969Z",
     "shell.execute_reply": "2024-12-07T18:01:10.047969Z",
     "shell.execute_reply.started": "2024-12-07T18:01:10.044982Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a79ce6e-788e-4ad3-811f-7f2cfe7968e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T18:01:11.552183Z",
     "iopub.status.busy": "2024-12-07T18:01:11.552183Z",
     "iopub.status.idle": "2024-12-07T18:01:11.555895Z",
     "shell.execute_reply": "2024-12-07T18:01:11.554891Z",
     "shell.execute_reply.started": "2024-12-07T18:01:11.552183Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbb008f2-d783-4e01-8741-78f7bca75f18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T18:01:12.723856Z",
     "iopub.status.busy": "2024-12-07T18:01:12.722856Z",
     "iopub.status.idle": "2024-12-07T18:01:15.543831Z",
     "shell.execute_reply": "2024-12-07T18:01:15.543831Z",
     "shell.execute_reply.started": "2024-12-07T18:01:12.723856Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25aa17a753da4d3ca296e5ea62aa3efc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Preetham\\miniconda3\\envs\\cs263_proj\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Preetham\\.cache\\huggingface\\hub\\models--llava-hf--llava-v1.6-mistral-7b-hf. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1241726dbe7347f7a4377e6b6e4d02d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.07k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8086a19699324e50beb2245b788c6654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52b2169950034d45a0562b6029e5c8b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "230be864bb284db78a48b56998e25e52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/41.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "202639cdac3f44869ef572e0a1b80afa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/552 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac9f8aa5b495452f986333e8bc635131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processor_config.json:   0%|          | 0.00/176 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f99c89aa54c4021bd5cf09d87434268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some kwargs in processor config are unused and will not have any effect: num_additional_image_tokens. \n"
     ]
    }
   ],
   "source": [
    "processor = LlavaNextProcessor.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4cf6d90-011a-4909-998e-6bab1f7d1678",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T18:01:18.377001Z",
     "iopub.status.busy": "2024-12-07T18:01:18.377001Z",
     "iopub.status.idle": "2024-12-07T18:08:59.814446Z",
     "shell.execute_reply": "2024-12-07T18:08:59.814446Z",
     "shell.execute_reply.started": "2024-12-07T18:01:18.377001Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4c3021dbde8429d9843d6959aadd2cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.28k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4974380df447452ca8e7131987215794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/70.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aa0d5483e804eedb503668f4e2ab117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b42b6692d55e49e4bb292e240255efc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ad5b15135b142b0b28ddc5126b7d998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fe0cdab14be496bbdc80b944097fa69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ec2aa66a9124018878d3a41c2db6ebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/380M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07ed6601895544009e65cc1161bbe1ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5832335856b946bbb6626eb1b8f3a3dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlavaNextForConditionalGeneration(\n",
       "  (vision_tower): CLIPVisionModel(\n",
       "    (vision_model): CLIPVisionTransformer(\n",
       "      (embeddings): CLIPVisionEmbeddings(\n",
       "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "        (position_embedding): Embedding(577, 1024)\n",
       "      )\n",
       "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (encoder): CLIPEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x CLIPEncoderLayer(\n",
       "            (self_attn): CLIPSdpaAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (multi_modal_projector): LlavaNextMultiModalProjector(\n",
       "    (linear_1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "    (act): GELUActivation()\n",
       "    (linear_2): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  )\n",
       "  (language_model): MistralForCausalLM(\n",
       "    (model): MistralModel(\n",
       "      (embed_tokens): Embedding(32064, 4096)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x MistralDecoderLayer(\n",
       "          (self_attn): MistralSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): MistralRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): MistralMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=4096, out_features=32064, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LlavaNextForConditionalGeneration.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\", torch_dtype=torch.float16, low_cpu_mem_usage=True, device_map=device) \n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e9d5ebc-a93a-43af-b341-b866ad5a2eca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T18:09:37.215273Z",
     "iopub.status.busy": "2024-12-07T18:09:37.215273Z",
     "iopub.status.idle": "2024-12-07T18:09:37.218010Z",
     "shell.execute_reply": "2024-12-07T18:09:37.218010Z",
     "shell.execute_reply.started": "2024-12-07T18:09:37.215273Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 4090\n",
      "Memory Usage:\n",
      "Allocated: 14.1 GB\n",
      "Cached:    14.2 GB\n"
     ]
    }
   ],
   "source": [
    "#check where the tensors are allocated\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62f6037d-d617-490b-95dd-12304946c496",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T18:09:38.552265Z",
     "iopub.status.busy": "2024-12-07T18:09:38.552265Z",
     "iopub.status.idle": "2024-12-07T18:09:38.554128Z",
     "shell.execute_reply": "2024-12-07T18:09:38.554128Z",
     "shell.execute_reply.started": "2024-12-07T18:09:38.552265Z"
    }
   },
   "outputs": [],
   "source": [
    "#this is to load pope\n",
    "# dataset = load_dataset(\"lmms-lab/POPE\", \"default\")\n",
    "# dataset = dataset['test'].filter(lambda x: x['category'] == 'adversarial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9808cbe-81fe-4c7b-a122-0a11ec95fd53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T18:09:40.747631Z",
     "iopub.status.busy": "2024-12-07T18:09:40.747631Z",
     "iopub.status.idle": "2024-12-07T18:09:44.064358Z",
     "shell.execute_reply": "2024-12-07T18:09:44.064358Z",
     "shell.execute_reply.started": "2024-12-07T18:09:40.747631Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#this is to load hallusion bench\n",
    "dataset = load_dataset(\"lmms-lab/HallusionBench\", \"default\")\n",
    "dataset = dataset['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "165d4719-7940-4b54-9037-ebcd5945c9c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T18:09:50.660639Z",
     "iopub.status.busy": "2024-12-07T18:09:50.660639Z",
     "iopub.status.idle": "2024-12-07T18:09:50.666063Z",
     "shell.execute_reply": "2024-12-07T18:09:50.665059Z",
     "shell.execute_reply.started": "2024-12-07T18:09:50.660639Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_response(question, image):\n",
    "    \"\"\" Prompt model with question regarding image and generate response.\n",
    "\n",
    "    Args:\n",
    "        question (str): question regarding the image content\n",
    "        image_path (str): PIL image object\n",
    "    \n",
    "    Returns:\n",
    "        response (str): model's response to the question\n",
    "    \"\"\"\n",
    "    inputs = processor(images=image, text=question, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs)\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a15ec993-56f2-41f6-b37c-92929d66e2a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T18:09:55.726378Z",
     "iopub.status.busy": "2024-12-07T18:09:55.726378Z",
     "iopub.status.idle": "2024-12-07T18:09:55.730363Z",
     "shell.execute_reply": "2024-12-07T18:09:55.730363Z",
     "shell.execute_reply.started": "2024-12-07T18:09:55.726378Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_response_new(question,image):\n",
    "    conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": question},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "    prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "    inputs = processor(image, prompt, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "    _, length_inputs = inputs[\"input_ids\"].shape\n",
    "    # print(\"length of inputs: \", length_inputs)\n",
    "    # autoregressively complete prompt\n",
    "    output = model.generate(**inputs, max_new_tokens=100)\n",
    "    output = output[:, length_inputs:]\n",
    "    \n",
    "    return processor.decode(output[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38c32ee1-a492-423f-9cbf-caa53ce26f0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-07T18:12:45.500595Z",
     "iopub.status.busy": "2024-12-07T18:12:45.500595Z",
     "iopub.status.idle": "2024-12-07T18:12:45.593495Z",
     "shell.execute_reply": "2024-12-07T18:12:45.593495Z",
     "shell.execute_reply.started": "2024-12-07T18:12:45.500595Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid input type. Check that `images` and/or `text` are valid inputs.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/Preetham/Desktop/images/Instagram-Post-2023-08-07T200711.295.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExplain about the image\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m response \u001b[38;5;241m=\u001b[39m generate_response_new(question, image)\n",
      "Cell \u001b[1;32mIn[11], line 12\u001b[0m, in \u001b[0;36mgenerate_response_new\u001b[1;34m(question, image)\u001b[0m\n\u001b[0;32m      2\u001b[0m     conversation \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      3\u001b[0m     {\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     },\n\u001b[0;32m     10\u001b[0m ]\n\u001b[0;32m     11\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mapply_chat_template(conversation, add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 12\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m processor(image, prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m     _, length_inputs \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# print(\"length of inputs: \", length_inputs)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# autoregressively complete prompt\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cs263_proj\\Lib\\site-packages\\transformers\\models\\llava_next\\processing_llava_next.py:122\u001b[0m, in \u001b[0;36mLlavaNextProcessor.__call__\u001b[1;34m(self, images, text, audio, videos, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to specify at least images or text.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# check if images and text inputs are reversed for BC\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m images, text \u001b[38;5;241m=\u001b[39m _validate_images_text_input_order(images, text)\n\u001b[0;32m    124\u001b[0m output_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_kwargs(\n\u001b[0;32m    125\u001b[0m     LlavaNextProcessorKwargs,\n\u001b[0;32m    126\u001b[0m     tokenizer_init_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39minit_kwargs,\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    128\u001b[0m )\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cs263_proj\\Lib\\site-packages\\transformers\\processing_utils.py:1180\u001b[0m, in \u001b[0;36m_validate_images_text_input_order\u001b[1;34m(images, text)\u001b[0m\n\u001b[0;32m   1174\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[0;32m   1175\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou may have used the wrong order for inputs. `images` should be passed before `text`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1176\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `images` and `text` inputs will be swapped. This behavior will be deprecated in transformers v4.47.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1177\u001b[0m     )\n\u001b[0;32m   1178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text, images\n\u001b[1;32m-> 1180\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input type. Check that `images` and/or `text` are valid inputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid input type. Check that `images` and/or `text` are valid inputs."
     ]
    }
   ],
   "source": [
    "image = \"/Users/Preetham/Desktop/images/Instagram-Post-2023-08-07T200711.295.png\"\n",
    "question = \"Explain about the image\"\n",
    "response = generate_response_new(question, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9d8787c-e897-4db2-b275-ef6df14e7f56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T04:07:13.415648Z",
     "iopub.status.busy": "2024-11-18T04:07:13.415648Z",
     "iopub.status.idle": "2024-11-18T04:07:13.420839Z",
     "shell.execute_reply": "2024-11-18T04:07:13.420839Z",
     "shell.execute_reply.started": "2024-11-18T04:07:13.415648Z"
    }
   },
   "outputs": [],
   "source": [
    "#access the RAM tags\n",
    "def tags_to_dict(filepath):\n",
    "    filepath = filepath\n",
    "    ram_data = {}\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip().rstrip(\",\")\n",
    "    \n",
    "            data_entry = json.loads(line)\n",
    "    \n",
    "            ram_data.update(data_entry)\n",
    "    return ram_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b17a856e-3f50-462f-94d9-a797caa89e83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T04:07:13.421842Z",
     "iopub.status.busy": "2024-11-18T04:07:13.420839Z",
     "iopub.status.idle": "2024-11-18T04:07:13.425217Z",
     "shell.execute_reply": "2024-11-18T04:07:13.425217Z",
     "shell.execute_reply.started": "2024-11-18T04:07:13.421842Z"
    }
   },
   "outputs": [],
   "source": [
    "pope_tag_path =  \"../../models/recognize-anything/pope_tags.json\"\n",
    "hallusion_tag_path = \"../../models/recognize-anything/hallusionBench_tags.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f76e9c92-8a1e-48df-b0a8-27756efe7ce5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T04:08:05.920339Z",
     "iopub.status.busy": "2024-11-18T04:08:05.920339Z",
     "iopub.status.idle": "2024-11-18T04:08:05.922854Z",
     "shell.execute_reply": "2024-11-18T04:08:05.922854Z",
     "shell.execute_reply.started": "2024-11-18T04:08:05.920339Z"
    }
   },
   "outputs": [],
   "source": [
    "ram_data = tags_to_dict(hallusion_tag_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af247376-024c-4fab-b3d4-10c37ae12108",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T04:07:13.431968Z",
     "iopub.status.busy": "2024-11-18T04:07:13.430965Z",
     "iopub.status.idle": "2024-11-18T04:07:13.434607Z",
     "shell.execute_reply": "2024-11-18T04:07:13.434607Z",
     "shell.execute_reply.started": "2024-11-18T04:07:13.431968Z"
    }
   },
   "outputs": [],
   "source": [
    "def obtain_attributes(img_src):\n",
    "    \"\"\"\n",
    "    Returns the attributes identified by RAM.\n",
    "    \"\"\"\n",
    "    injection = \"This image has these attributes: \"\n",
    "    image_attrs = ram_data[img_src]\n",
    "    image_attrs = image_attrs.replace('|',' ').split()\n",
    "    injection = \"This image has these attributes: \"\n",
    "    for i in range(len(image_attrs)):\n",
    "        if i == len(image_attrs) - 2:\n",
    "            injection = injection + image_attrs[i]+\", and \"\n",
    "        elif i == len(image_attrs) - 1:\n",
    "            injection = injection + image_attrs[i] +\". \"\n",
    "        else:\n",
    "            injection = injection + image_attrs[i] + \", \"\n",
    "    return injection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97cebc5f-6d24-44b5-a649-694e01d74ed9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T04:07:13.434607Z",
     "iopub.status.busy": "2024-11-18T04:07:13.434607Z",
     "iopub.status.idle": "2024-11-18T04:07:13.439697Z",
     "shell.execute_reply": "2024-11-18T04:07:13.439697Z",
     "shell.execute_reply.started": "2024-11-18T04:07:13.434607Z"
    }
   },
   "outputs": [],
   "source": [
    "def inject_info(img_src, question):\n",
    "    \"\"\"\n",
    "    Injects prompt with any needed information. So given question, it will tell the lvlm also what it contains.\n",
    "    Should fine-tune prompt later.\n",
    "    \"\"\"\n",
    "    image_attrs = ram_data[img_src]\n",
    "    image_attrs = image_attrs.replace('|',' ').split()\n",
    "    injection = obtain_attributes(img_src)\n",
    "    injection = injection + f\"Using this information answer the following question: {question}\"\n",
    "    return injection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96bbf1fe-3e00-4285-9872-09a6826ee7a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T04:08:11.555417Z",
     "iopub.status.busy": "2024-11-18T04:08:11.555417Z",
     "iopub.status.idle": "2024-11-18T04:08:22.157118Z",
     "shell.execute_reply": "2024-11-18T04:08:22.157118Z",
     "shell.execute_reply.started": "2024-11-18T04:08:11.555417Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expanding inputs for image tokens in LLaVa-NeXT should be done in processing. Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Expanding inputs for image tokens in LLaVa-NeXT should be done in processing. Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image has these attributes: graph, number, and individual. Using this information answer the following question: Is China, Hongkong SAR, the leading importing country of gold, silverware, and jewelry with the highest import value in 2018? : The image you've provided shows a bar chart comparing the import value of gold, silverware, and jewelry among several countries in 2018. According to the chart, China, Hong Kong SAR, is indeed the leading importer of these goods, with an import value of $10,000. This is significantly higher than the import values of other countries listed on the chart. \n"
     ]
    }
   ],
   "source": [
    "responses = []\n",
    "for idx in range(len(dataset)):\n",
    "    question = dataset['question'][idx]\n",
    "    image = dataset['image'][idx]\n",
    "    img_source = dataset['filename'][idx] #this is for hallusion bench\n",
    "    # img_source = dataset['image_source'][idx]#this is for pope\n",
    "    prompt = inject_info(img_source, question)\n",
    "    response = generate_response_new(question, image)\n",
    "    responses.append({\n",
    "        'question': question,\n",
    "        'response': response\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697cff7b-1e57-4ad6-8c7c-466d3c5b7467",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-18T04:07:21.052298Z",
     "iopub.status.idle": "2024-11-18T04:07:21.052298Z",
     "shell.execute_reply": "2024-11-18T04:07:21.052298Z",
     "shell.execute_reply.started": "2024-11-18T04:07:21.052298Z"
    }
   },
   "outputs": [],
   "source": [
    "llava_pope_output_path = os.path.join(os.curdir, \"internvl_pope_responses.json\")\n",
    "llava_hallusion_output_path =  os.path.join(os.curdir, \"internvl_hallusion_responses.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8bf184-deb0-4d7b-8b9f-49bf01cdf8e2",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-11-18T04:07:21.053301Z",
     "iopub.status.idle": "2024-11-18T04:07:21.053301Z",
     "shell.execute_reply": "2024-11-18T04:07:21.053301Z",
     "shell.execute_reply.started": "2024-11-18T04:07:21.053301Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write responses to file\n",
    "output_path = llava_pope_output_path\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(responses, f, indent=4)\n",
    "\n",
    "print(f\"LLaVa's responses have been saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
